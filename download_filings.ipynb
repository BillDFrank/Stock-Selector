{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"  \"\n",
    "idx_dir = \"data/edgar\"  # Replace with your directory\n",
    "filings_list = []\n",
    "\n",
    "for year in range(1994, 2026):\n",
    "    for qtr in [\"QTR1\", \"QTR2\", \"QTR3\", \"QTR4\"]:\n",
    "        idx_file = f\"{idx_dir}/{year}/{qtr}/master.idx\"\n",
    "        if os.path.exists(idx_file):\n",
    "            with open(idx_file, 'r') as f:\n",
    "                lines = f.readlines()[11:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split('|')\n",
    "                if len(parts) >= 5 and parts[2] in ['10-K']:\n",
    "                    filings_list.append({\n",
    "                        'CIK': parts[0],\n",
    "                        'Company': parts[1],\n",
    "                        'Form': parts[2],\n",
    "                        'Date': parts[3],\n",
    "                        'URL': base_url + parts[4]\n",
    "                    })\n",
    "\n",
    "filings_df = pd.DataFrame(filings_list)\n",
    "filings_df.to_csv('data/filings_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1995-03-27:   1%|          | 2465/228416 [32:46<68:02:47,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.sec.gov/Archives/edgar/data/66904/0000092122-95-000038.txt - Status: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1996-12-23:   3%|▎         | 7797/228416 [1:54:18<59:23:23,  1.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.sec.gov/Archives/edgar/data/1005697/0000912057-96-030332.txt - Status: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1997-03-31:   5%|▍         | 11098/228416 [2:45:33<51:21:29,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped data/edgar/filings/839947/1997/10-K_1997-03-31.txt - already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1998-03-30:   8%|▊         | 17798/228416 [4:27:44<118:06:57,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.sec.gov/Archives/edgar/data/810830/0000810830-98-000001.txt - Status: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1998-02-13:   8%|▊         | 18157/228416 [4:33:14<464:01:04,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.sec.gov/Archives/edgar/data/846930/0000950132-98-000259.txt - Status: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1998-03-11:   8%|▊         | 18330/228416 [4:36:15<591:53:12, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.sec.gov/Archives/edgar/data/864601/0000892569-98-000878.txt - Status: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1998-03-20:   8%|▊         | 19338/228416 [4:51:00<214:21:02,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.sec.gov/Archives/edgar/data/929900/0000950005-98-000343.txt - Status: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_1998-03-31:   8%|▊         | 19385/228416 [4:52:04<449:04:39,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.sec.gov/Archives/edgar/data/933590/0000933590-98-000002.txt - Status: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_2002-04-01:  20%|██        | 46299/228416 [11:09:17<42:35:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped data/edgar/filings/81350/2002/10-K_2002-04-01.txt - already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_2007-03-28:  39%|███▊      | 87990/228416 [21:05:19<32:08:09,  1.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped data/edgar/filings/811785/2007/10-K_2007-03-28.txt - already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_2008-03-31:  41%|████▏     | 94397/228416 [22:38:37<30:56:56,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped data/edgar/filings/1347185/2008/10-K_2008-03-31.txt - already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_2008-03-31:  43%|████▎     | 97200/228416 [23:18:18<30:37:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped data/edgar/filings/819975/2008/10-K_2008-03-31.txt - already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 10-K_2009-03-31:  45%|████▌     | 102930/228416 [24:45:23<30:34:24,  1.14it/s]"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "filings_df = pd.read_csv('data/filings_list.csv')\n",
    "download_dir = \"data/edgar/filings\"\n",
    "headers = {'User-Agent': 'WilliamFrank william_dieter@hotmail.com'}\n",
    "\n",
    "# Assuming filings_df is defined elsewhere\n",
    "total_files = len(filings_df)\n",
    "\n",
    "# Wrap iterrows() with tqdm for progress bar\n",
    "with tqdm(filings_df.iterrows(), total=total_files, desc=\"Downloading Filings\") as pbar:\n",
    "    for index, row in pbar:\n",
    "        cik, form, date, url = row['CIK'], row['Form'], row['Date'], row['URL']\n",
    "        year = date[:4]\n",
    "        save_dir = f\"{download_dir}/{cik}/{year}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        file_path = f\"{save_dir}/{form}_{date}.txt\"\n",
    "        \n",
    "        # Check if file already exists and is not empty\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            # Only print skips every 100 files to reduce spam\n",
    "            if index % 100 == 0:\n",
    "                pbar.write(f\"Skipped {file_path} - already exists\")\n",
    "            continue\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            # Update tqdm description with current file (optional, less spammy)\n",
    "            pbar.set_description(f\"Downloading {form}_{date}\") \n",
    "        else:\n",
    "            # Always print failures for debugging\n",
    "            pbar.write(f\"Failed to download {url} - Status: {response.status_code}\")\n",
    "        \n",
    "        time.sleep(0.1)  # Rate limit delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths (adjust as needed)\n",
    "project_root = os.getcwd()\n",
    "json_file = os.path.join(project_root, \"data\", \"edgar\", \"company_tickers_exchange.json\")\n",
    "txt_file = os.path.join(project_root, \"data\", \"edgar\", \"ticker.txt\")\n",
    "output_file = os.path.join(project_root, \"data\", \"cik_ticker_mapping.csv\")\n",
    "\n",
    "# Step 1: Load your CIK list (example assumes from a directory; adjust as needed)\n",
    "filings_dir = os.path.join(project_root, \"data\", \"edgar\", \"filings\")\n",
    "cik_folders = [folder for folder in os.listdir(filings_dir) \n",
    "               if os.path.isdir(os.path.join(filings_dir, folder))]\n",
    "cik_list = [f\"{int(cik):010d}\" for cik in cik_folders]  # Standardize to 10-digit strings\n",
    "my_ciks_df = pd.DataFrame({\"cik\": cik_list})\n",
    "\n",
    "# Step 2: Load JSON file\n",
    "with open(json_file, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Step 3: Inspect JSON structure\n",
    "tickers_list = json_data.get(\"data\", [])\n",
    "if not tickers_list:\n",
    "    raise ValueError(\"No 'data' key found in JSON file or data is empty.\")\n",
    "print(\"First entry in JSON data:\", tickers_list[0])  # Inspect the first entry\n",
    "\n",
    "# Step 4: Convert JSON data to DataFrame\n",
    "json_df = pd.DataFrame(tickers_list, columns=[\"cik\", \"name\", \"ticker\", \"exchange\"])\n",
    "\n",
    "# Standardize CIK to 10-digit strings\n",
    "json_df[\"cik\"] = json_df[\"cik\"].apply(lambda x: f\"{int(x):010d}\" if isinstance(x, int) else x.zfill(10))\n",
    "\n",
    "# Step 5: Load TXT file for fallback\n",
    "txt_df = pd.read_csv(txt_file, sep=\"\\t\", header=None, names=[\"ticker\", \"cik\"], dtype=str)\n",
    "txt_df[\"cik\"] = txt_df[\"cik\"].apply(lambda x: x.zfill(10))\n",
    "\n",
    "# Step 6: Merge with JSON data (primary source)\n",
    "merged_json = my_ciks_df.merge(json_df, on=\"cik\", how=\"left\")\n",
    "\n",
    "# Step 7: Handle CIKs not found in JSON\n",
    "not_found_ciks = merged_json[merged_json[\"ticker\"].isna()][\"cik\"].unique()\n",
    "not_found_df = pd.DataFrame({\"cik\": not_found_ciks})\n",
    "merged_txt = not_found_df.merge(txt_df, on=\"cik\", how=\"left\")\n",
    "merged_txt[\"name\"] = None  # No name available from TXT\n",
    "merged_txt[\"exchange\"] = None  # No exchange available from TXT\n",
    "\n",
    "# Step 8: Combine results\n",
    "found = merged_json.dropna(subset=[\"ticker\"])\n",
    "final_df = pd.concat([found, merged_txt], ignore_index=True)\n",
    "\n",
    "# Step 9: Fill missing tickers\n",
    "final_df[\"ticker\"] = final_df[\"ticker\"].fillna(\"Not Found\")\n",
    "\n",
    "# Step 10: Order columns\n",
    "final_df = final_df[[\"cik\", \"name\", \"ticker\", \"exchange\"]]\n",
    "\n",
    "# Step 11: Save to CSV\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved mapping for {len(final_df)} rows to {output_file}\")\n",
    "\n",
    "# Preview the result\n",
    "print(\"\\nPreview of the mapping:\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
